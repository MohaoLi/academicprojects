{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>18</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.7255</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>20</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.7502</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>21</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.7867</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>22</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>0.7888</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.7408</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>960.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1583.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>60.6</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>10</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>827.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>0.7552</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>12</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>774.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>66.8</td>\n",
       "      <td>0.7951</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>13</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>869.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1537.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>76.4</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>14</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.1</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>15</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>933.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1647.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>79.8</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>16</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>912.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>71.2</td>\n",
       "      <td>0.8569</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>17</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1383.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>67.6</td>\n",
       "      <td>0.8185</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>21</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.7419</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>805.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>0.6438</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>829.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1378.0</td>\n",
       "      <td>832.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>59.7</td>\n",
       "      <td>0.6308</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>10</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>41.7</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>11</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>969.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>986.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1537.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1593.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0.6262</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>14</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>15</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>16</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>37.2</td>\n",
       "      <td>0.6657</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>17</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>994.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>18</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>39.6</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>19</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>43.4</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2004-03-12</td>\n",
       "      <td>22</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1484.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>839.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>59.1</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2004-03-13</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>964.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1544.0</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>64.1</td>\n",
       "      <td>0.7597</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2004-03-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>873.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>63.9</td>\n",
       "      <td>0.7423</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>936.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>611.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.6168</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>715.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>30.7</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>10</td>\n",
       "      <td>1.4</td>\n",
       "      <td>970.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>677.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>888.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>11</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>779.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>12</td>\n",
       "      <td>1.4</td>\n",
       "      <td>996.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>781.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.5594</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>701.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.5302</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>722.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>956.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>783.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>896.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5809</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>16</td>\n",
       "      <td>1.3</td>\n",
       "      <td>968.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>826.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>29.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.4192</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>17</td>\n",
       "      <td>1.4</td>\n",
       "      <td>953.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>817.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.3866</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>18</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>743.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.5945</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>19</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>20</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>894.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5814</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>21</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>46.6</td>\n",
       "      <td>0.8642</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5815</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>22</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>803.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>2005-04-03</td>\n",
       "      <td>23</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>769.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>0.8497</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>683.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.8275</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>579.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>51.2</td>\n",
       "      <td>0.8058</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>912.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>544.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>56.3</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>887.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>508.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>974.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>58.9</td>\n",
       "      <td>0.7888</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>888.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>528.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>59.9</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>730.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>905.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>63.1</td>\n",
       "      <td>0.7531</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.7446</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5824</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5825</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>1583.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.7487</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>10</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.7568</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>11</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>12</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.6406</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>13</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>961.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5139</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>2005-04-04</td>\n",
       "      <td>14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5831 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Time  CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "0     2004-03-10    18     2.6       1360.0      11.9         1046.0    166.0   \n",
       "1     2004-03-10    19     2.0       1292.0       9.4          955.0    103.0   \n",
       "2     2004-03-10    20     2.2       1402.0       9.0          939.0    131.0   \n",
       "3     2004-03-10    21     2.2       1376.0       9.2          948.0    172.0   \n",
       "4     2004-03-10    22     1.6       1272.0       6.5          836.0    131.0   \n",
       "5     2004-03-11     8     2.0       1333.0       8.0          900.0    174.0   \n",
       "6     2004-03-11     9     2.2       1351.0       9.5          960.0    129.0   \n",
       "7     2004-03-11    10     1.7       1233.0       6.3          827.0    112.0   \n",
       "8     2004-03-11    12     1.6       1236.0       5.2          774.0    104.0   \n",
       "9     2004-03-11    13     1.9       1286.0       7.3          869.0    146.0   \n",
       "10    2004-03-11    14     2.9       1371.0      11.5         1034.0    207.0   \n",
       "11    2004-03-11    15     2.2       1310.0       8.8          933.0    184.0   \n",
       "12    2004-03-11    16     2.2       1292.0       8.3          912.0    193.0   \n",
       "13    2004-03-11    17     2.9       1383.0      11.2         1020.0    243.0   \n",
       "14    2004-03-11    21     3.9       1313.0      12.8         1076.0    240.0   \n",
       "15    2004-03-12     0     1.7       1080.0       5.9          805.0    122.0   \n",
       "16    2004-03-12     1     1.9       1044.0       6.4          829.0    133.0   \n",
       "17    2004-03-12    10     3.1       1350.0      14.0         1118.0    187.0   \n",
       "18    2004-03-12    11     2.7       1263.0      11.6         1037.0    216.0   \n",
       "19    2004-03-12    12     2.1       1206.0      10.2          986.0    143.0   \n",
       "20    2004-03-12    13     2.5       1252.0      11.0         1016.0    160.0   \n",
       "21    2004-03-12    14     2.7       1287.0      12.8         1078.0    163.0   \n",
       "22    2004-03-12    15     2.9       1353.0      14.2         1122.0    190.0   \n",
       "23    2004-03-12    16     2.8       1309.0      12.7         1073.0    178.0   \n",
       "24    2004-03-12    17     2.4       1274.0      11.7         1041.0    150.0   \n",
       "25    2004-03-12    18     3.9       1510.0      19.3         1277.0    206.0   \n",
       "26    2004-03-12    19     3.7       1525.0      18.2         1246.0    202.0   \n",
       "27    2004-03-12    22     3.5       1484.0      14.3         1127.0    253.0   \n",
       "28    2004-03-13     0     2.7       1280.0       9.6          964.0    193.0   \n",
       "29    2004-03-13     1     1.9       1196.0       7.4          873.0    139.0   \n",
       "...          ...   ...     ...          ...       ...            ...      ...   \n",
       "5801  2005-04-03     8     0.7        936.0       2.4          611.0    134.0   \n",
       "5802  2005-04-03     9     1.2       1022.0       4.1          715.0    192.0   \n",
       "5803  2005-04-03    10     1.4        970.0       3.4          677.0    166.0   \n",
       "5804  2005-04-03    11     1.3       1000.0       5.3          779.0    171.0   \n",
       "5805  2005-04-03    12     1.4        996.0       5.3          781.0    177.0   \n",
       "5806  2005-04-03    13     1.0        928.0       3.8          701.0    119.0   \n",
       "5807  2005-04-03    14     1.0        933.0       4.2          722.0    121.0   \n",
       "5808  2005-04-03    15     1.1        956.0       5.4          783.0    142.0   \n",
       "5809  2005-04-03    16     1.3        968.0       6.3          826.0    197.0   \n",
       "5810  2005-04-03    17     1.4        953.0       6.1          817.0    242.0   \n",
       "5811  2005-04-03    18     1.2       1015.0       4.6          743.0    190.0   \n",
       "5812  2005-04-03    19     2.7       1248.0      11.1         1018.0    367.0   \n",
       "5813  2005-04-03    20     2.5       1180.0       7.9          894.0    355.0   \n",
       "5814  2005-04-03    21     1.5       1102.0       6.0          812.0    235.0   \n",
       "5815  2005-04-03    22     1.6       1116.0       5.8          803.0    233.0   \n",
       "5816  2005-04-03    23     1.2       1100.0       5.1          769.0    170.0   \n",
       "5817  2005-04-04     0     0.9       1012.0       3.5          683.0    117.0   \n",
       "5818  2005-04-04     1     0.6        944.0       1.9          579.0     70.0   \n",
       "5819  2005-04-04     2     0.5        912.0       1.5          544.0     69.0   \n",
       "5820  2005-04-04     3     0.4        887.0       1.1          508.0     62.0   \n",
       "5821  2005-04-04     5     0.5        888.0       1.3          528.0     77.0   \n",
       "5822  2005-04-04     6     1.1       1031.0       4.4          730.0    182.0   \n",
       "5823  2005-04-04     7     4.0       1384.0      17.4         1221.0    594.0   \n",
       "5824  2005-04-04     8     5.0       1446.0      22.4         1362.0    586.0   \n",
       "5825  2005-04-04     9     3.9       1297.0      13.6         1102.0    523.0   \n",
       "5826  2005-04-04    10     3.1       1314.0      13.5         1101.0    472.0   \n",
       "5827  2005-04-04    11     2.4       1163.0      11.4         1027.0    353.0   \n",
       "5828  2005-04-04    12     2.4       1142.0      12.4         1063.0    293.0   \n",
       "5829  2005-04-04    13     2.1       1003.0       9.5          961.0    235.0   \n",
       "5830  2005-04-04    14     2.2       1071.0      11.9         1047.0    265.0   \n",
       "\n",
       "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  Temp    RH      AH  \\\n",
       "0           1056.0    113.0        1692.0       1268.0  13.6  48.9  0.7578   \n",
       "1           1174.0     92.0        1559.0        972.0  13.3  47.7  0.7255   \n",
       "2           1140.0    114.0        1555.0       1074.0  11.9  54.0  0.7502   \n",
       "3           1092.0    122.0        1584.0       1203.0  11.0  60.0  0.7867   \n",
       "4           1205.0    116.0        1490.0       1110.0  11.2  59.6  0.7888   \n",
       "5           1136.0    112.0        1517.0       1102.0  10.8  57.4  0.7408   \n",
       "6           1079.0    101.0        1583.0       1028.0  10.5  60.6  0.7691   \n",
       "7           1218.0     98.0        1446.0        860.0  10.8  58.4  0.7552   \n",
       "8           1301.0     95.0        1401.0        664.0   9.5  66.8  0.7951   \n",
       "9           1162.0    112.0        1537.0        799.0   8.3  76.4  0.8393   \n",
       "10           983.0    128.0        1730.0       1037.0   8.0  81.1  0.8736   \n",
       "11          1082.0    126.0        1647.0        946.0   8.3  79.8  0.8778   \n",
       "12          1103.0    131.0        1591.0        957.0   9.7  71.2  0.8569   \n",
       "13          1008.0    135.0        1719.0       1104.0   9.8  67.6  0.8185   \n",
       "14           957.0    136.0        1707.0       1285.0   9.1  64.0  0.7419   \n",
       "15          1254.0     97.0        1375.0        816.0   8.3  58.5  0.6438   \n",
       "16          1247.0    110.0        1378.0        832.0   7.7  59.7  0.6308   \n",
       "17           912.0    122.0        1712.0       1237.0  13.2  41.7  0.6320   \n",
       "18           969.0    143.0        1598.0       1167.0  14.3  38.4  0.6243   \n",
       "19          1035.0    113.0        1537.0        959.0  15.0  36.5  0.6195   \n",
       "20          1008.0    116.0        1593.0        983.0  16.1  34.5  0.6262   \n",
       "21           949.0    123.0        1660.0       1061.0  16.3  35.7  0.6560   \n",
       "22           922.0    126.0        1740.0       1139.0  15.8  37.0  0.6610   \n",
       "23           954.0    120.0        1657.0       1112.0  15.9  37.2  0.6657   \n",
       "24          1006.0    119.0        1610.0        994.0  16.9  34.3  0.6549   \n",
       "25           812.0    149.0        1910.0       1410.0  15.1  39.6  0.6766   \n",
       "26           821.0    145.0        1847.0       1448.0  14.4  43.4  0.7084   \n",
       "27           839.0    139.0        1723.0       1491.0  11.0  59.1  0.7740   \n",
       "28           963.0    113.0        1544.0       1285.0   9.5  64.1  0.7597   \n",
       "29          1071.0     97.0        1463.0       1144.0   9.1  63.9  0.7423   \n",
       "...            ...      ...           ...          ...   ...   ...     ...   \n",
       "5801         911.0     89.0         949.0        741.0  13.5  40.0  0.6168   \n",
       "5802         806.0    118.0        1004.0        831.0  17.8  30.7  0.6205   \n",
       "5803         888.0    113.0         931.0        613.0  21.1  23.7  0.5875   \n",
       "5804         805.0    115.0        1001.0        640.0  24.0  19.3  0.5673   \n",
       "5805         806.0    124.0         985.0        631.0  26.5  16.4  0.5594   \n",
       "5806         926.0     86.0         902.0        456.0  28.7  13.7  0.5302   \n",
       "5807         899.0     87.0         890.0        450.0  28.5  13.1  0.5002   \n",
       "5808         857.0    100.0         896.0        516.0  30.0  11.1  0.4624   \n",
       "5809         867.0    132.0         898.0        548.0  29.4  10.4  0.4192   \n",
       "5810         872.0    156.0         891.0        603.0  28.9   9.9  0.3866   \n",
       "5811         851.0    138.0         981.0        597.0  22.8  21.7  0.5945   \n",
       "5812         599.0    181.0        1289.0       1167.0  19.9  33.0  0.7608   \n",
       "5813         636.0    187.0        1200.0       1372.0  17.5  40.7  0.8073   \n",
       "5814         693.0    158.0        1178.0       1042.0  16.4  46.6  0.8642   \n",
       "5815         696.0    153.0        1173.0       1055.0  15.5  49.0  0.8579   \n",
       "5816         722.0    128.0        1147.0       1049.0  14.3  52.5  0.8497   \n",
       "5817         801.0     93.0        1073.0        816.0  14.2  51.4  0.8275   \n",
       "5818         925.0     58.0        1002.0        598.0  13.8  51.2  0.8058   \n",
       "5819         959.0     55.0        1002.0        573.0  12.1  56.3  0.7927   \n",
       "5820        1047.0     51.0         974.0        549.0  11.3  58.9  0.7888   \n",
       "5821        1077.0     53.0         987.0        578.0  10.4  59.9  0.7550   \n",
       "5822         760.0     93.0        1129.0        905.0   9.5  63.1  0.7531   \n",
       "5823         470.0    155.0        1600.0       1457.0   9.7  61.9  0.7446   \n",
       "5824         415.0    174.0        1777.0       1705.0  13.5  48.9  0.7553   \n",
       "5825         507.0    187.0        1375.0       1583.0  18.2  36.3  0.7487   \n",
       "5826         539.0    190.0        1374.0       1729.0  21.9  29.3  0.7568   \n",
       "5827         604.0    179.0        1264.0       1269.0  24.3  23.7  0.7119   \n",
       "5828         603.0    175.0        1241.0       1092.0  26.9  18.3  0.6406   \n",
       "5829         702.0    156.0        1041.0        770.0  28.3  13.5  0.5139   \n",
       "5830         654.0    168.0        1129.0        816.0  28.5  13.1  0.5028   \n",
       "\n",
       "            Day  Month  \n",
       "0     Wednesday      3  \n",
       "1     Wednesday      3  \n",
       "2     Wednesday      3  \n",
       "3     Wednesday      3  \n",
       "4     Wednesday      3  \n",
       "5      Thursday      3  \n",
       "6      Thursday      3  \n",
       "7      Thursday      3  \n",
       "8      Thursday      3  \n",
       "9      Thursday      3  \n",
       "10     Thursday      3  \n",
       "11     Thursday      3  \n",
       "12     Thursday      3  \n",
       "13     Thursday      3  \n",
       "14     Thursday      3  \n",
       "15       Friday      3  \n",
       "16       Friday      3  \n",
       "17       Friday      3  \n",
       "18       Friday      3  \n",
       "19       Friday      3  \n",
       "20       Friday      3  \n",
       "21       Friday      3  \n",
       "22       Friday      3  \n",
       "23       Friday      3  \n",
       "24       Friday      3  \n",
       "25       Friday      3  \n",
       "26       Friday      3  \n",
       "27       Friday      3  \n",
       "28     Saturday      3  \n",
       "29     Saturday      3  \n",
       "...         ...    ...  \n",
       "5801     Sunday      4  \n",
       "5802     Sunday      4  \n",
       "5803     Sunday      4  \n",
       "5804     Sunday      4  \n",
       "5805     Sunday      4  \n",
       "5806     Sunday      4  \n",
       "5807     Sunday      4  \n",
       "5808     Sunday      4  \n",
       "5809     Sunday      4  \n",
       "5810     Sunday      4  \n",
       "5811     Sunday      4  \n",
       "5812     Sunday      4  \n",
       "5813     Sunday      4  \n",
       "5814     Sunday      4  \n",
       "5815     Sunday      4  \n",
       "5816     Sunday      4  \n",
       "5817     Monday      4  \n",
       "5818     Monday      4  \n",
       "5819     Monday      4  \n",
       "5820     Monday      4  \n",
       "5821     Monday      4  \n",
       "5822     Monday      4  \n",
       "5823     Monday      4  \n",
       "5824     Monday      4  \n",
       "5825     Monday      4  \n",
       "5826     Monday      4  \n",
       "5827     Monday      4  \n",
       "5828     Monday      4  \n",
       "5829     Monday      4  \n",
       "5830     Monday      4  \n",
       "\n",
       "[5831 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# Loading data\n",
    "air_quality_data = pd.read_csv('data.csv')\n",
    "air_quality_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use hierarchical clustering to classify my whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2167, 4: 1362, 3: 1057, 2: 878, 1: 367})\n",
      "[2 2 2 ... 3 0 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c0e2914e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hierarchical: Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter\n",
    "\n",
    "def set_colors(labels, colors='rgbykcmw'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        if (label < 0) or (label > 6):\n",
    "          colored_labels.append(colors[7]) \n",
    "        else:\n",
    "          colored_labels.append(colors[label])\n",
    "    return colored_labels\n",
    "\n",
    "# Fit an estimator\n",
    "estimator = AgglomerativeClustering(n_clusters=5)\n",
    "X = air_quality_data[[\"CO(GT)\",\"PT08.S1(CO)\"]]\n",
    "estimator.fit(X)\n",
    "# Clusters are given in the labels_ attribute\n",
    "labels = estimator.labels_\n",
    "print(Counter(labels))\n",
    "print(labels)\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(air_quality_data[\"CO(GT)\"], air_quality_data[\"PT08.S1(CO)\"], c=colors)\n",
    "plt.xlabel(\"CO(GT)\")\n",
    "plt.ylabel(\"PT08.S1(CO)\")\n",
    "plt.show()\n",
    "\n",
    "air_quality_data['labels'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "Supervised learning infers a function from labeled training data consisting of a set of training examples.: Predict a dependent variable using at least four classes of base supervised learners.  \n",
    "  \n",
    "#### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation # used to test classifier\n",
    "from sklearn.cross_validation import KFold, cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import roc_curve # ROC Curves\n",
    "from sklearn.metrics import auc # Calculating AUC for ROC's!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5831, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.477323</td>\n",
       "      <td>1.346399</td>\n",
       "      <td>0.383899</td>\n",
       "      <td>0.502395</td>\n",
       "      <td>-0.415022</td>\n",
       "      <td>1.073040</td>\n",
       "      <td>-0.025922</td>\n",
       "      <td>0.913746</td>\n",
       "      <td>0.635986</td>\n",
       "      <td>-0.370805</td>\n",
       "      <td>-0.042533</td>\n",
       "      <td>-0.483855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.027792</td>\n",
       "      <td>0.990627</td>\n",
       "      <td>-0.034916</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>-0.763763</td>\n",
       "      <td>1.603089</td>\n",
       "      <td>-0.491001</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>-0.176726</td>\n",
       "      <td>-0.408133</td>\n",
       "      <td>-0.113606</td>\n",
       "      <td>-0.570478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140579</td>\n",
       "      <td>1.566141</td>\n",
       "      <td>-0.101926</td>\n",
       "      <td>0.030914</td>\n",
       "      <td>-0.608767</td>\n",
       "      <td>1.450363</td>\n",
       "      <td>-0.003775</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>0.103330</td>\n",
       "      <td>-0.582332</td>\n",
       "      <td>0.259527</td>\n",
       "      <td>-0.504237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140579</td>\n",
       "      <td>1.430111</td>\n",
       "      <td>-0.068421</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>-0.381808</td>\n",
       "      <td>1.234750</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.457518</td>\n",
       "      <td>-0.694317</td>\n",
       "      <td>0.614892</td>\n",
       "      <td>-0.406349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.364536</td>\n",
       "      <td>0.885988</td>\n",
       "      <td>-0.520741</td>\n",
       "      <td>-0.422941</td>\n",
       "      <td>-0.608767</td>\n",
       "      <td>1.742339</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.278382</td>\n",
       "      <td>0.202173</td>\n",
       "      <td>-0.669431</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>-0.400717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)   NOx(GT)  PT08.S3(NOx)  \\\n",
       "0  0.477323     1.346399  0.383899       0.502395 -0.415022      1.073040   \n",
       "1 -0.027792     0.990627 -0.034916       0.101416 -0.763763      1.603089   \n",
       "2  0.140579     1.566141 -0.101926       0.030914 -0.608767      1.450363   \n",
       "3  0.140579     1.430111 -0.068421       0.070571 -0.381808      1.234750   \n",
       "4 -0.364536     0.885988 -0.520741      -0.422941 -0.608767      1.742339   \n",
       "\n",
       "    NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)      Temp        RH        AH  \n",
       "0 -0.025922      0.913746     0.635986 -0.370805 -0.042533 -0.483855  \n",
       "1 -0.491001      0.495413    -0.176726 -0.408133 -0.113606 -0.570478  \n",
       "2 -0.003775      0.482831     0.103330 -0.582332  0.259527 -0.504237  \n",
       "3  0.173398      0.574047     0.457518 -0.694317  0.614892 -0.406349  \n",
       "4  0.040518      0.278382     0.202173 -0.669431  0.591201 -0.400717  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "air_scaled = air_quality_data.iloc[:, 2:14].apply(scale)\n",
    "air_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062999</td>\n",
       "      <td>0.222807</td>\n",
       "      <td>0.053542</td>\n",
       "      <td>0.079123</td>\n",
       "      <td>-0.061454</td>\n",
       "      <td>0.101522</td>\n",
       "      <td>-0.003728</td>\n",
       "      <td>0.180775</td>\n",
       "      <td>0.101907</td>\n",
       "      <td>-0.079895</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>-0.112550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003668</td>\n",
       "      <td>0.163932</td>\n",
       "      <td>-0.004870</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>-0.113093</td>\n",
       "      <td>0.151671</td>\n",
       "      <td>-0.070607</td>\n",
       "      <td>0.098012</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.087938</td>\n",
       "      <td>-0.024342</td>\n",
       "      <td>-0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.259171</td>\n",
       "      <td>-0.014215</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>-0.090142</td>\n",
       "      <td>0.137221</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>0.095523</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>-0.125472</td>\n",
       "      <td>0.055608</td>\n",
       "      <td>-0.117292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.236660</td>\n",
       "      <td>-0.009543</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>-0.056536</td>\n",
       "      <td>0.116822</td>\n",
       "      <td>0.024935</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.073310</td>\n",
       "      <td>-0.149600</td>\n",
       "      <td>0.131750</td>\n",
       "      <td>-0.094522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.048113</td>\n",
       "      <td>0.146616</td>\n",
       "      <td>-0.072627</td>\n",
       "      <td>-0.066609</td>\n",
       "      <td>-0.090142</td>\n",
       "      <td>0.164845</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.032395</td>\n",
       "      <td>-0.144238</td>\n",
       "      <td>0.126674</td>\n",
       "      <td>-0.093212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)   NOx(GT)  PT08.S3(NOx)  \\\n",
       "0  0.062999     0.222807  0.053542       0.079123 -0.061454      0.101522   \n",
       "1 -0.003668     0.163932 -0.004870       0.015972 -0.113093      0.151671   \n",
       "2  0.018554     0.259171 -0.014215       0.004869 -0.090142      0.137221   \n",
       "3  0.018554     0.236660 -0.009543       0.011114 -0.056536      0.116822   \n",
       "4 -0.048113     0.146616 -0.072627      -0.066609 -0.090142      0.164845   \n",
       "\n",
       "    NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)      Temp        RH        AH  \n",
       "0 -0.003728      0.180775     0.101907 -0.079895 -0.009113 -0.112550  \n",
       "1 -0.070607      0.098012    -0.028318 -0.087938 -0.024342 -0.132700  \n",
       "2 -0.000543      0.095523     0.016557 -0.125472  0.055608 -0.117292  \n",
       "3  0.024935      0.113569     0.073310 -0.149600  0.131750 -0.094522  \n",
       "4  0.005827      0.055075     0.032395 -0.144238  0.126674 -0.093212  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.mean()) / (df.max() - df.min())\n",
    "air_normalized = air_quality_data.iloc[:, 2: 14].apply(normalize)\n",
    "air_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = air_normalized\n",
    "# setting target\n",
    "y = air_quality_data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing data to have a training and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal K is 5\n"
     ]
    }
   ],
   "source": [
    "knn_k = []\n",
    "for i in range(0, 33): # try up to k = 33\n",
    "    if (i % 2 != 0):\n",
    "        knn_k.append(i)\n",
    "\n",
    "cross_vals = []\n",
    "for k in knn_k:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cross_vals.append(scores.mean())\n",
    "    \n",
    "MSE = [1 - x for x in cross_vals]\n",
    "optimal_k = knn_k[MSE.index(min(MSE))]\n",
    "print(\"Optimal K is {0}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using kNN is 0.7095452134401319\n",
      "Accuracy score using kNN is 0.8298328332618946\n"
     ]
    }
   ],
   "source": [
    "# setting kNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# kNN cross validation\n",
    "\n",
    "print(\"KfoldCrossVal mean score using kNN is %s\" %cross_val_score(knn, X, y, cv=10).mean())\n",
    "\n",
    "# kNN metrics\n",
    "kNNm = knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = kNNm.predict(X_test)\n",
    "print(\"Accuracy score using kNN is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions for each supervised learner:  \n",
    "* Which hyper-parameters are important?  \n",
    "  \n",
    "The n_neighbors is an important hyper-parameters, because KNN depends on the number of closest neighbors to vote, and then the data is decided which class it belongs to.\n",
    "* What hyper-parameter values work best?   \n",
    "  \n",
    "From the optimal method, I can know that when n_neigbors is 5, the cross-validation score is best, which is why 5 of n_neighbors works best.\n",
    "* Which supervised learner works best on the test data?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines(SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ('linear', 'rbf'), 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1,2,3,4,5,6,7,8,9,10]}\n",
    "svr = svm.SVC()\n",
    "grid = GridSearchCV(svr, parameters)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9559748427672956"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using SVM is 0.963664280787156\n",
      "Accuracy score using SVM is 0.9751393056150879\n"
     ]
    }
   ],
   "source": [
    "#setting svm classifier\n",
    "svc = grid.best_estimator_\n",
    "\n",
    "print(\"KfoldCrossVal mean score using SVM is %s\" %cross_val_score(svc,X,y,cv=10).mean())\n",
    "#SVM metrics\n",
    "sm = svc.fit(X_train, y_train)\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using SVM is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions for each supervised learner:  \n",
    "* Which hyper-parameters are important?  \n",
    "  \n",
    "I think the kernal is important, because the SVMs depends on the kernel function to process data and, then classify input.\n",
    "* What hyper-parameter values work best?   \n",
    "  \n",
    "I use grid search to look for the best hyper-parameter, so I get kernel is linear when the whole model works best.\n",
    "* Which supervised learner works best on the test data?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using Decision Tree is 0.9996563533301842\n",
      "Accuracy score using Decision Tree is 0.9995713673381912\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree classifier\n",
    "\n",
    "DTm = DecisionTreeClassifier()\n",
    "                                \n",
    "# Decision Tree cross validation\n",
    "\n",
    "print(\"KfoldCrossVal mean score using Decision Tree is %s\" %cross_val_score(DTm,X,y,cv=10).mean())\n",
    "\n",
    "# Decision Tree metrics\n",
    "sm = DTm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using Decision Tree is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KfoldCrossVal mean score using Random Forest is 0.999485413159244\n",
      "Accuracy score using Random Forest is 0.9995713673381912\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "RFm = RandomForestClassifier(random_state = 42, \n",
    "                                criterion='gini',\n",
    "                                n_estimators = 500,\n",
    "                                max_features = 5)\n",
    "                                \n",
    "# Random Forest cross validation\n",
    "print(\"KfoldCrossVal mean score using Random Forest is %s\" %cross_val_score(RFm,X,y,cv=10).mean())\n",
    "\n",
    "# Random Forest metrics\n",
    "sm = RFm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sm.predict(X_test)\n",
    "print(\"Accuracy score using Random Forest is %s\" %metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions for each supervised learner:  \n",
    "* Which hyper-parameters are important?  \n",
    "  \n",
    "I think n_estimators and max_features are important hyper-parameters, because random forest is based on the decision tree. n_estimators is the number of decision trees in the forest, and the max_features is maximum of features to split input in a decision tree.\n",
    "* What hyper-parameter values work best?  \n",
    "  \n",
    "I think the values I set already are pretty good, because the validataion score is almost 1.\n",
    "* Which supervised learner works best on the test data?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score using Naive Bayes is 0.8401200171453065\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_class = gnb.predict(X_test)\n",
    "print (\"Accuracy score using Naive Bayes is %s\" %metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions for each supervised learner:  \n",
    "* Which hyper-parameters are important?  \n",
    "  \n",
    "I think there is no important hyper-parameters in the gaussian naive bayes model.\n",
    "* What hyper-parameter values work best?   \n",
    "  \n",
    "Same as above, there is no best values of hyper-parameters.\n",
    "* Which supervised learner works best on the test data?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which supervised learner works best on the test data?    \n",
    "From all the validation scores I get above, I think the random forest works best on the test data. Although the validation score of decision tree is almost same as the random forest, the random forest avoids some cons of decision trees like overfitting, so I think the random forest would be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "Neural networks can be used for supervised learning: Predict a dependent variable using a feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3498, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_net(n = 20, i = 12, o = 5):\n",
    "    # create simple one dense layer net\n",
    "    # default 20 neurons, input 12, output 5\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation=\"relu\", input_shape=(i,)))\n",
    "    net.add(Dense(50, activation=\"relu\"))\n",
    "    net.add(Dense(150, activation=\"relu\"))\n",
    "    net.add(Dense(150, activation=\"relu\"))\n",
    "    net.add(Dense(150, activation=\"relu\"))\n",
    "    net.add(Dense(150, activation=\"relu\"))\n",
    "    net.add(Dense(50, activation=\"relu\"))\n",
    "    net.add(Dense(1, activation=\"relu\"))\n",
    "    #\n",
    "    net.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=0.0002), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = shallow_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 20)                260       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 150)               7650      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 84,511\n",
      "Trainable params: 84,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3498 samples, validate on 2333 samples\n",
      "Epoch 1/80\n",
      "3498/3498 [==============================] - 2s 471us/step - loss: 2.1910 - acc: 0.2956 - val_loss: 1.1785 - val_acc: 0.4243\n",
      "Epoch 2/80\n",
      "3498/3498 [==============================] - 1s 311us/step - loss: 0.9423 - acc: 0.5197 - val_loss: 0.8422 - val_acc: 0.5624\n",
      "Epoch 3/80\n",
      "3498/3498 [==============================] - 1s 317us/step - loss: 0.8091 - acc: 0.5663 - val_loss: 0.7722 - val_acc: 0.6005\n",
      "Epoch 4/80\n",
      "3498/3498 [==============================] - 1s 318us/step - loss: 0.6520 - acc: 0.6415 - val_loss: 0.6690 - val_acc: 0.6657\n",
      "Epoch 5/80\n",
      "3498/3498 [==============================] - 1s 320us/step - loss: 0.5681 - acc: 0.6718 - val_loss: 0.5701 - val_acc: 0.6892\n",
      "Epoch 6/80\n",
      "3498/3498 [==============================] - 1s 338us/step - loss: 0.5044 - acc: 0.6941 - val_loss: 0.6263 - val_acc: 0.6918\n",
      "Epoch 7/80\n",
      "3498/3498 [==============================] - 1s 329us/step - loss: 0.4570 - acc: 0.7264 - val_loss: 0.5229 - val_acc: 0.7432\n",
      "Epoch 8/80\n",
      "3498/3498 [==============================] - 1s 381us/step - loss: 0.3922 - acc: 0.7716 - val_loss: 0.5011 - val_acc: 0.7578\n",
      "Epoch 9/80\n",
      "3498/3498 [==============================] - 1s 367us/step - loss: 0.3689 - acc: 0.7770 - val_loss: 0.4925 - val_acc: 0.7934\n",
      "Epoch 10/80\n",
      "3498/3498 [==============================] - 1s 382us/step - loss: 0.3465 - acc: 0.8047 - val_loss: 0.4793 - val_acc: 0.7998\n",
      "Epoch 11/80\n",
      "3498/3498 [==============================] - 1s 335us/step - loss: 0.3042 - acc: 0.8305 - val_loss: 0.3461 - val_acc: 0.8341\n",
      "Epoch 12/80\n",
      "3498/3498 [==============================] - 1s 334us/step - loss: 0.3045 - acc: 0.8276 - val_loss: 0.3552 - val_acc: 0.7758\n",
      "Epoch 13/80\n",
      "3498/3498 [==============================] - 1s 324us/step - loss: 0.2763 - acc: 0.8448 - val_loss: 0.3863 - val_acc: 0.7827\n",
      "Epoch 14/80\n",
      "3498/3498 [==============================] - 1s 348us/step - loss: 0.2445 - acc: 0.8619 - val_loss: 0.4422 - val_acc: 0.8345\n",
      "Epoch 15/80\n",
      "3498/3498 [==============================] - 1s 367us/step - loss: 0.2646 - acc: 0.8642 - val_loss: 0.4608 - val_acc: 0.7934\n",
      "Epoch 16/80\n",
      "3498/3498 [==============================] - 1s 384us/step - loss: 0.2394 - acc: 0.8659 - val_loss: 0.3159 - val_acc: 0.8560\n",
      "Epoch 17/80\n",
      "3498/3498 [==============================] - 1s 326us/step - loss: 0.2262 - acc: 0.8756 - val_loss: 0.2908 - val_acc: 0.8547\n",
      "Epoch 18/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.2259 - acc: 0.8688 - val_loss: 0.2665 - val_acc: 0.8453\n",
      "Epoch 19/80\n",
      "3498/3498 [==============================] - 1s 329us/step - loss: 0.2092 - acc: 0.8868 - val_loss: 0.2902 - val_acc: 0.8796\n",
      "Epoch 20/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.2272 - acc: 0.8696 - val_loss: 0.2997 - val_acc: 0.8847\n",
      "Epoch 21/80\n",
      "3498/3498 [==============================] - 1s 395us/step - loss: 0.2052 - acc: 0.8891 - val_loss: 0.2854 - val_acc: 0.8710\n",
      "Epoch 22/80\n",
      "3498/3498 [==============================] - 1s 394us/step - loss: 0.1961 - acc: 0.8977 - val_loss: 0.2721 - val_acc: 0.8808\n",
      "Epoch 23/80\n",
      "3498/3498 [==============================] - 1s 428us/step - loss: 0.1880 - acc: 0.8968 - val_loss: 0.2426 - val_acc: 0.8813\n",
      "Epoch 24/80\n",
      "3498/3498 [==============================] - 1s 350us/step - loss: 0.2147 - acc: 0.8856 - val_loss: 0.3386 - val_acc: 0.8041\n",
      "Epoch 25/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.1850 - acc: 0.8991 - val_loss: 0.2331 - val_acc: 0.8967\n",
      "Epoch 26/80\n",
      "3498/3498 [==============================] - 1s 369us/step - loss: 0.1703 - acc: 0.8979 - val_loss: 0.2848 - val_acc: 0.8868\n",
      "Epoch 27/80\n",
      "3498/3498 [==============================] - 1s 332us/step - loss: 0.1791 - acc: 0.9011 - val_loss: 0.2299 - val_acc: 0.8881\n",
      "Epoch 28/80\n",
      "3498/3498 [==============================] - 1s 368us/step - loss: 0.1536 - acc: 0.9097 - val_loss: 0.2448 - val_acc: 0.8916\n",
      "Epoch 29/80\n",
      "3498/3498 [==============================] - 1s 328us/step - loss: 0.1830 - acc: 0.9065 - val_loss: 0.2542 - val_acc: 0.8834\n",
      "Epoch 30/80\n",
      "3498/3498 [==============================] - 1s 341us/step - loss: 0.1491 - acc: 0.9131 - val_loss: 0.2567 - val_acc: 0.8903\n",
      "Epoch 31/80\n",
      "3498/3498 [==============================] - 1s 332us/step - loss: 0.1784 - acc: 0.9008 - val_loss: 0.2927 - val_acc: 0.8834\n",
      "Epoch 32/80\n",
      "3498/3498 [==============================] - 1s 339us/step - loss: 0.1376 - acc: 0.9228 - val_loss: 0.2211 - val_acc: 0.8868\n",
      "Epoch 33/80\n",
      "3498/3498 [==============================] - 1s 367us/step - loss: 0.1677 - acc: 0.9108 - val_loss: 0.2305 - val_acc: 0.8671\n",
      "Epoch 34/80\n",
      "3498/3498 [==============================] - 2s 441us/step - loss: 0.1522 - acc: 0.9142 - val_loss: 0.2353 - val_acc: 0.9018\n",
      "Epoch 35/80\n",
      "3498/3498 [==============================] - 2s 467us/step - loss: 0.1403 - acc: 0.9200 - val_loss: 0.2936 - val_acc: 0.9040\n",
      "Epoch 36/80\n",
      "3498/3498 [==============================] - 1s 422us/step - loss: 0.1579 - acc: 0.9222 - val_loss: 0.2557 - val_acc: 0.8997\n",
      "Epoch 37/80\n",
      "3498/3498 [==============================] - 1s 395us/step - loss: 0.1335 - acc: 0.9171 - val_loss: 0.2882 - val_acc: 0.9113\n",
      "Epoch 38/80\n",
      "3498/3498 [==============================] - 1s 403us/step - loss: 0.1338 - acc: 0.9348 - val_loss: 0.2822 - val_acc: 0.9100\n",
      "Epoch 39/80\n",
      "3498/3498 [==============================] - 1s 424us/step - loss: 0.2067 - acc: 0.9168 - val_loss: 0.4195 - val_acc: 0.8916\n",
      "Epoch 40/80\n",
      "3498/3498 [==============================] - 2s 461us/step - loss: 0.1378 - acc: 0.9274 - val_loss: 0.2776 - val_acc: 0.9070\n",
      "Epoch 41/80\n",
      "3498/3498 [==============================] - 1s 392us/step - loss: 0.1473 - acc: 0.9265 - val_loss: 0.2325 - val_acc: 0.8984\n",
      "Epoch 42/80\n",
      "3498/3498 [==============================] - 1s 369us/step - loss: 0.1213 - acc: 0.9348 - val_loss: 0.3087 - val_acc: 0.9138\n",
      "Epoch 43/80\n",
      "3498/3498 [==============================] - 1s 372us/step - loss: 0.1317 - acc: 0.9248 - val_loss: 0.4601 - val_acc: 0.8924\n",
      "Epoch 44/80\n",
      "3498/3498 [==============================] - 2s 442us/step - loss: 0.1147 - acc: 0.9320 - val_loss: 0.2040 - val_acc: 0.9143\n",
      "Epoch 45/80\n",
      "3498/3498 [==============================] - 1s 404us/step - loss: 0.1409 - acc: 0.9405 - val_loss: 0.2467 - val_acc: 0.9211\n",
      "Epoch 46/80\n",
      "3498/3498 [==============================] - 1s 389us/step - loss: 0.1835 - acc: 0.9200 - val_loss: 0.2539 - val_acc: 0.8924\n",
      "Epoch 47/80\n",
      "3498/3498 [==============================] - 1s 367us/step - loss: 0.0977 - acc: 0.9383 - val_loss: 0.3369 - val_acc: 0.9143\n",
      "Epoch 48/80\n",
      "3498/3498 [==============================] - 1s 356us/step - loss: 0.1394 - acc: 0.9274 - val_loss: 0.2309 - val_acc: 0.8856\n",
      "Epoch 49/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.1328 - acc: 0.9274 - val_loss: 0.2252 - val_acc: 0.8967\n",
      "Epoch 50/80\n",
      "3498/3498 [==============================] - 2s 448us/step - loss: 0.1170 - acc: 0.9405 - val_loss: 0.1896 - val_acc: 0.9147\n",
      "Epoch 51/80\n",
      "3498/3498 [==============================] - 1s 418us/step - loss: 0.1141 - acc: 0.9411 - val_loss: 0.2760 - val_acc: 0.8894\n",
      "Epoch 52/80\n",
      "3498/3498 [==============================] - 1s 385us/step - loss: 0.1274 - acc: 0.9423 - val_loss: 0.3752 - val_acc: 0.9130\n",
      "Epoch 53/80\n",
      "3498/3498 [==============================] - 1s 360us/step - loss: 0.1213 - acc: 0.9405 - val_loss: 0.2537 - val_acc: 0.8710\n",
      "Epoch 54/80\n",
      "3498/3498 [==============================] - 1s 375us/step - loss: 0.0956 - acc: 0.9477 - val_loss: 0.1708 - val_acc: 0.9258\n",
      "Epoch 55/80\n",
      "3498/3498 [==============================] - 1s 375us/step - loss: 0.1008 - acc: 0.9451 - val_loss: 0.2418 - val_acc: 0.9130\n",
      "Epoch 56/80\n",
      "3498/3498 [==============================] - 2s 444us/step - loss: 0.0912 - acc: 0.9497 - val_loss: 0.3732 - val_acc: 0.9040\n",
      "Epoch 57/80\n",
      "3498/3498 [==============================] - 2s 445us/step - loss: 0.1158 - acc: 0.9388 - val_loss: 0.3182 - val_acc: 0.9134\n",
      "Epoch 58/80\n",
      "3498/3498 [==============================] - 2s 456us/step - loss: 0.1084 - acc: 0.9514 - val_loss: 0.3165 - val_acc: 0.8976\n",
      "Epoch 59/80\n",
      "3498/3498 [==============================] - 2s 445us/step - loss: 0.1386 - acc: 0.9354 - val_loss: 0.2480 - val_acc: 0.8508\n",
      "Epoch 60/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498/3498 [==============================] - 2s 434us/step - loss: 0.0951 - acc: 0.9500 - val_loss: 0.2186 - val_acc: 0.9250\n",
      "Epoch 61/80\n",
      "3498/3498 [==============================] - 2s 451us/step - loss: 0.0930 - acc: 0.9517 - val_loss: 0.2384 - val_acc: 0.9331\n",
      "Epoch 62/80\n",
      "3498/3498 [==============================] - 2s 445us/step - loss: 0.0945 - acc: 0.9503 - val_loss: 0.2061 - val_acc: 0.9288\n",
      "Epoch 63/80\n",
      "3498/3498 [==============================] - 2s 433us/step - loss: 0.1278 - acc: 0.9488 - val_loss: 0.5064 - val_acc: 0.8817\n",
      "Epoch 64/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.0822 - acc: 0.9554 - val_loss: 0.2468 - val_acc: 0.9276\n",
      "Epoch 65/80\n",
      "3498/3498 [==============================] - 1s 325us/step - loss: 0.0918 - acc: 0.9548 - val_loss: 0.1715 - val_acc: 0.9331\n",
      "Epoch 66/80\n",
      "3498/3498 [==============================] - 1s 339us/step - loss: 0.0977 - acc: 0.9523 - val_loss: 0.1883 - val_acc: 0.9173\n",
      "Epoch 67/80\n",
      "3498/3498 [==============================] - 1s 365us/step - loss: 0.0825 - acc: 0.9523 - val_loss: 0.2078 - val_acc: 0.9177\n",
      "Epoch 68/80\n",
      "3498/3498 [==============================] - 1s 373us/step - loss: 0.0854 - acc: 0.9597 - val_loss: 0.1882 - val_acc: 0.9314\n",
      "Epoch 69/80\n",
      "3498/3498 [==============================] - 1s 377us/step - loss: 0.0794 - acc: 0.9543 - val_loss: 0.1930 - val_acc: 0.9310\n",
      "Epoch 70/80\n",
      "3498/3498 [==============================] - 1s 409us/step - loss: 0.1078 - acc: 0.9551 - val_loss: 0.1644 - val_acc: 0.9318\n",
      "Epoch 71/80\n",
      "3498/3498 [==============================] - 1s 340us/step - loss: 0.0989 - acc: 0.9551 - val_loss: 0.2617 - val_acc: 0.9126\n",
      "Epoch 72/80\n",
      "3498/3498 [==============================] - 1s 340us/step - loss: 0.0732 - acc: 0.9620 - val_loss: 0.2226 - val_acc: 0.9301\n",
      "Epoch 73/80\n",
      "3498/3498 [==============================] - 1s 338us/step - loss: 0.0806 - acc: 0.9545 - val_loss: 0.2537 - val_acc: 0.9237\n",
      "Epoch 74/80\n",
      "3498/3498 [==============================] - 1s 349us/step - loss: 0.1422 - acc: 0.9494 - val_loss: 0.2152 - val_acc: 0.9280\n",
      "Epoch 75/80\n",
      "3498/3498 [==============================] - 1s 358us/step - loss: 0.0972 - acc: 0.9540 - val_loss: 0.2239 - val_acc: 0.9280\n",
      "Epoch 76/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.0737 - acc: 0.9620 - val_loss: 0.2043 - val_acc: 0.9353\n",
      "Epoch 77/80\n",
      "3498/3498 [==============================] - 1s 353us/step - loss: 0.1040 - acc: 0.9597 - val_loss: 0.1922 - val_acc: 0.9336\n",
      "Epoch 78/80\n",
      "3498/3498 [==============================] - 1s 348us/step - loss: 0.0806 - acc: 0.9657 - val_loss: 0.2574 - val_acc: 0.9387\n",
      "Epoch 79/80\n",
      "3498/3498 [==============================] - 1s 344us/step - loss: 0.0628 - acc: 0.9651 - val_loss: 0.2222 - val_acc: 0.9323\n",
      "Epoch 80/80\n",
      "3498/3498 [==============================] - 2s 434us/step - loss: 0.0756 - acc: 0.9611 - val_loss: 0.2219 - val_acc: 0.9340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9c192a86d8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=10, epochs=80, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333/2333 [==============================] - 0s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22185905687463064, 0.9339905703624737]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions for each supervised learner:  \n",
    "* Which hyper-parameters are important?  \n",
    "  \n",
    "The number of neuron, activation function and the number of layers are important. The activation function is to decide when each neuron fires, if there is no a appropriate or good activation function, the whole neural network would easily die.\n",
    "* What hyper-parameter values work best?   \n",
    "  \n",
    "I think the hyper-parameter values I provide above are good, because the validation score is over 0.93.\n",
    "* How the the neural network compare to the supervised learners in part A?   \n",
    "  \n",
    "Obviously, the validation score of supervised learners in Part A are better than neural network, but I think the neural network would be better than the supervised learners in Part A. Because I could run the program more times to improve the accuracy of the model, and the neural network has more potentiality to make progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "Create a stacked ensemble super-model.  \n",
    "Generate a stacked ensemble super-model for the best learners in parts A & B: Did the stacked ensemble super-model help? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
